{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ageWlWGwW182"
      },
      "source": [
        "# Screening for ligands using machine learning\n",
        "\n",
        "**Note:** This tutorial is based on T7 from [TeachOpenCADD](https://doi.org/10.1186/s13321-019-0351-x).\n",
        "\n",
        "Authors:\n",
        "\n",
        "* Albert J. Kooistra, 2022, [University of Copenhagen](https://drug.ku.dk/staff/?pure=en/persons/612712)\n",
        "* Andrea Volkamer, 2019-2020, [Volkamer lab](https://volkamerlab.org)\n",
        "* Talia B. Kimber, 2019-2020, [Volkamer lab](https://volkamerlab.org)\n",
        "* Jacob Gora, CADD seminar 2018, Charité/FU Berlin\n",
        "* Jan Philipp Albrecht, CADD seminar 2018, Charité/FU Berlin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCzpoTsxW186"
      },
      "source": [
        "## Aim of this practical\n",
        "\n",
        "Machine learning (ML) has gained quite a momentum in drug discovery and especially in ligand-based virtual screening / predictive modeling. In this tutorial, you will learn how to use different supervised ML algorithms to predict the activity of novel compounds against a protein target of interest (EGFR) based on a training set of known active and inactive compounds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8octtQWNW187"
      },
      "source": [
        "### Techniques you will use:\n",
        "\n",
        "* Data preparation:\n",
        "    * Fingerprint encoding of small molecules\n",
        "* Supervised Machine learning\n",
        "    * Random forest classifier\n",
        "    * Support vector classifier\n",
        "    * Neural network classifier\n",
        "* Model validation and evaluation\n",
        "    * Validation strategy: K-fold cross-validation\n",
        "    * Performance measures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro4KqcBvW188"
      },
      "source": [
        "## Theory\n",
        "\n",
        "To successfully apply ML, we need a large data set of molecules, a molecular encoding, a label per molecule in the data set, and a ML algorithm to train a model. Then, we can make predictions for new molecules.\n",
        "\n",
        "![ML overview](https://github.com/AJK-dev/course_materials/blob/main/Ligand_based_machine_learning/images/ML_overview.png?raw=1)\n",
        "\n",
        "_Figure 1_: Machine learning overview: Molecular encoding, label, ML algorithm, prediction. Figure by Andrea Volkamer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORyD5MZ8W188"
      },
      "source": [
        "### Data preparation: Molecule encoding\n",
        "\n",
        "For ML, molecules need to be converted into a list of features. Often molecular fingerprints are used as representation. \n",
        "\n",
        "The fingerprints used in this practical as implemented in RDKit (more info can be found in a  [presentation by G. Landrum](https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf)) are:\n",
        "\n",
        "* **maccs**: 'MACCS keys are 166 bit structural key descriptors in which each bit is associated with a SMARTS pattern.' (see OpenEye's `MACCS` [docs](https://docs.eyesopen.com/toolkits/python/graphsimtk/fingerprint.html#maccs))\n",
        "* **Morgan fingerprints** (and **ECFP**): 'Extended-Connectivity Fingerprints (ECFPs) are circular topological fingerprints designed for molecular characterization, similarity searching, and structure-activity modeling.' (see ChemAxon's `ECFP` [docs](https://docs.chemaxon.com/display/docs/Extended+Connectivity+Fingerprint+ECFP)) The original implementation of the ECFPs was done in Pipeline Pilot which is not open-source. Instead we use the implementation from RDKit which is called Morgan fingerprint. The two most important parameters of these fingerprints are the radius and fingerprint length. The first specifies the radius of circular neighborhoods considered for each atom. Here two radii are considered: 2 and 3. The length parameter specifies the length to which the bit string representation is hashed. The default length is 2048."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQcxMV3ZW189"
      },
      "source": [
        "### Machine learning (ML)\n",
        "\n",
        "ML can be applied for (text adapted from [scikit-learn page](http://scikit-learn.org/stable/)):\n",
        "\n",
        "* **Classification (supervised)**: Identify which category an object belongs to (e.g. : Nearest neighbors, Naive Bayes, RF, SVM, ...)\n",
        "* Regression: Prediction of a continuous-values attribute associated with an object\n",
        "* Clustering (unsupervised): Automated grouping of similar objects into sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWIri-6VW189"
      },
      "source": [
        "#### Supervised learning\n",
        "\n",
        "A learning algorithm creates rules by finding patterns in the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPv9a7nqW18-"
      },
      "source": [
        "* **Random Forest (RF)**: Ensemble of decision trees. A single decision tree splits the features of the input vector in a way that maximizes an objective function. In the random forest algorithm, the trees that are grown are de-correlated because the choice of features for the splits are chosen randomly.\n",
        "* **Support Vector Machines (SVMs)**: SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. The classifier is based on the idea of maximizing the margin as the objective function.  \n",
        "* **Artificial neural networks (ANNs)**: An ANN is based on a collection of connected units or nodes called artificial neurons which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it.\n",
        "\n",
        "\n",
        "![ANN_example](https://github.com/AJK-dev/course_materials/blob/main/Ligand_based_machine_learning/images/ANN_wiki.png?raw=1)\n",
        "\n",
        "_Figure 2_: Example of a neural network with one hidden layer. Figure taken from [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10d68za6W18-"
      },
      "source": [
        "### Model validation and evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aita1vUJW18_"
      },
      "source": [
        "#### Validation strategy: k-fold cross validation\n",
        "\n",
        "* This model validation technique splits the dataset in *k* parts/folds in an iterative manner:\n",
        "    * Training data set: *k-1* folds are considered as the known dataset on which the model is trained\n",
        "    * Test dataset: The unknown fold is used to validate the model\n",
        "    * Process is repeated for each of the folds\n",
        "* The goal is to test the ability of the model to predict data which it has never seen before in order to flag problems known as over-fitting and to assess the generalization ability of the model.\n",
        "\n",
        "![K-fold cross validation overview](https://github.com/AJK-dev/course_materials/blob/main/Ligand_based_machine_learning/images/kfold_validation.png?raw=1)\n",
        "\n",
        "_Figure 2_: Schematic depiction of k-fold cross validation. Figure by [Knowledge Transfer](https://androidkt.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rKng33FW18_"
      },
      "source": [
        "#### Performance measures\n",
        "\n",
        "* **Sensitivity**, also true positive rate\n",
        "    * TPR = TP/(FN + TP) \n",
        "    * _Intuitively_: Out of all actual positives, how many were predicted as positive?\n",
        "* **Specificity**, also true negative rate\n",
        "    * TNR = TN/(FP + TN)\n",
        "    * _Intuitively_: Out of all actual negatives, how many were predicted as negative?\n",
        "* **Accuracy**, also the trueness\n",
        "    * ACC = (TP + TN)/(TP + TN + FP + FN)\n",
        "    * _Intuitively_: Proportion of correct predictions.\n",
        "* **ROC-curve**, receiver operating characteristic curve\n",
        "    * A graphical plot that illustrates the diagnostic ability of our classifier\n",
        "    * Plots the sensitivity against the specificity\n",
        "* **AUC**, the area under the ROC curve (AUC):  \n",
        "    * Describes the probability that a classifier will rank a randomly chosen positive instance higher than a negative one\n",
        "    * Values between 0 and 1, the higher the better"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9bWe7hxW19A"
      },
      "source": [
        "| What the model predicts  | True active  |  True inactive |\n",
        "|---|---|---|\n",
        "| active  |  True Positive (TP) |  False Positive (FP) |\n",
        "| inactive  |  False Negative (FN) |  True Negative (TN) |"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why are there so many metrics? Couldn't we, for example, simply use the *Sensitivity* and skip the rest?"
      ],
      "metadata": {
        "id": "jqh3vDv7bLul"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXxA7eaNW19A"
      },
      "source": [
        "## Practical"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let us start setting up the environment\n",
        "\n",
        "First, we will setup our python environment by installing all the libraries that we need to process all data, generate the fingerprints and to create our machine learning models.\n",
        "\n",
        "Note that some of the cells in this notebook are \"hidden\" by default, click on *View* at the top then click on *Expand Sections* to show all cells. For slower computers, this might be a bit too much but you can also hide the cells again by clicking on *Collapse sections*."
      ],
      "metadata": {
        "id": "HMcsPJBpYov1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdkit-pypi -qqq"
      ],
      "metadata": {
        "id": "OSlnV9hYXjb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnwyfpSzW19B"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from warnings import filterwarnings\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import svm, metrics, clone\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from sklearn.metrics import auc, accuracy_score, recall_score\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import MACCSkeys\n",
        "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
        "\n",
        "# Silence some expected warnings\n",
        "filterwarnings(\"ignore\")\n",
        "\n",
        "# Fix seed for reproducible results\n",
        "SEED = 22"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQcAN9KzW19C"
      },
      "outputs": [],
      "source": [
        "# Set path to this notebook and create data folder\n",
        "HERE = Path(_dh[-1])\n",
        "DATA = HERE / \"data\"\n",
        "!mkdir \"data\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZEU8lDeW19D"
      },
      "source": [
        "### Load compound and activity data\n",
        "\n",
        "Let's start by loading our data, which focuses on the Epidermal growth factor receptor (EGFR) kinase. The *csv* file from a repository is loaded into a dataframe with the important columns:\n",
        "\n",
        "* CHEMBL-ID\n",
        "* SMILES string of the corresponding compound\n",
        "* Measured affinity: pIC50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZMMR_ZJW19D"
      },
      "outputs": [],
      "source": [
        "# Read EGFR inhibitor data\n",
        "chembl_df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/AJK-dev/course_materials/main/Ligand_based_machine_learning/data/EGFR_compounds_lipinski.csv\",\n",
        "    index_col=0,\n",
        ")\n",
        "\n",
        "# Look at head\n",
        "print(\"Shape of dataframe : \", chembl_df.shape)\n",
        "chembl_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkBBJ5DFW19F"
      },
      "outputs": [],
      "source": [
        "# Keep only the columns we want\n",
        "chembl_df = chembl_df[[\"molecule_chembl_id\", \"smiles\", \"pIC50\"]]\n",
        "chembl_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why would we only keep these columns? Is more data not always better?"
      ],
      "metadata": {
        "id": "v_JerKmDaEKc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jN6-sR_W19G"
      },
      "source": [
        "### Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUvTCKw5W19G"
      },
      "source": [
        "#### Data labeling\n",
        "We need to classify each compound as active or inactive. Therefore, we use the pIC50 value.\n",
        "\n",
        "* pIC50 = -log10(IC50) \n",
        "* IC50 describes the amount of substance needed to inhibit, _in vitro_, a process by 50% .\n",
        "* The pIC50 value we use to split the data differs from target to target and also depends on the data availability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qXGTpG6W19H"
      },
      "outputs": [],
      "source": [
        "# Add column for activity\n",
        "chembl_df[\"active\"] = 0\n",
        "\n",
        "# Mark every molecule as active with an pIC50 of 6.3 (i.e. IC50 <= 500 nM), 0 otherwise\n",
        "chembl_df.loc[chembl_df[chembl_df.pIC50 >= 6.3].index, \"active\"] = 1.0\n",
        "\n",
        "print(\"Number of active compounds:\", int(chembl_df.active.sum()))\n",
        "print(\"Number of inactive compounds:\", len(chembl_df) - int(chembl_df.active.sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** What are the limitations and dangers of splitting a dataset in such a way? What are potential pitfalls? "
      ],
      "metadata": {
        "id": "fmID0PZnafKz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoqZkh9QW19I"
      },
      "source": [
        "#### Molecule encoding\n",
        "\n",
        "Now we define a function `smiles_to_fp` to generate fingerprints from SMILES.\n",
        "For now, we incorporated the choice between the following fingerprints:\n",
        "\n",
        "* maccs\n",
        "* morgan2 and morgan3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVc-L6_SW19I"
      },
      "outputs": [],
      "source": [
        "def smiles_to_fp(smiles, method=\"maccs\", n_bits=2048):\n",
        "    \"\"\"\n",
        "    Encode a molecule from a SMILES string into a fingerprint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    smiles : str\n",
        "        The SMILES string defining the molecule.\n",
        "\n",
        "    method : str\n",
        "        The type of fingerprint to use. Default is MACCS keys.\n",
        "\n",
        "    n_bits : int\n",
        "        The length of the fingerprint.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    array\n",
        "        The fingerprint array.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # convert smiles to RDKit mol object\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "    if method == \"maccs\":\n",
        "        return np.array(MACCSkeys.GenMACCSKeys(mol))\n",
        "    if method == \"morgan2\":\n",
        "        return np.array(GetMorganFingerprintAsBitVect(mol, 2, nBits=n_bits))\n",
        "    if method == \"morgan3\":\n",
        "        return np.array(GetMorganFingerprintAsBitVect(mol, 3, nBits=n_bits))\n",
        "    else:\n",
        "        print(f\"Warning: Wrong method specified: {method}. Default will be used instead.\")\n",
        "        return np.array(MACCSkeys.GenMACCSKeys(mol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUYQPhYEW19J"
      },
      "outputs": [],
      "source": [
        "compound_df = chembl_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wixrUs7sW19J"
      },
      "outputs": [],
      "source": [
        "# Add column for fingerprint\n",
        "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp)\n",
        "compound_df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why are we using the Morgan fingerprint two times in our function? What is the difference between *morgan2* and *morgan3* (see slides)?\n",
        "\n",
        "**Question:** When we applied the fingerprint function, we did not specify a specific fingerprint. Which fingerprint is now present in our *compound_df* variable? "
      ],
      "metadata": {
        "id": "2wSZQGbab6rT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMmbb0uPW19K"
      },
      "source": [
        "### Machine Learning (ML)\n",
        "\n",
        "In the following, we will try several ML approaches to classify our molecules. We will use:\n",
        "\n",
        "* Random Forest (RF)\n",
        "* Support Vector Machine (SVM) \n",
        "* Artificial Neural Network (ANN) \n",
        "\n",
        "Additionally, we will comment on the results.\n",
        "\n",
        "The goal is to test the ability of the model to predict data which it has never seen before in order to flag problems known as over fitting and to assess the generalization ability of the model.\n",
        "\n",
        "We start by defining a function `model_training_and_validation` which fits a model on a random train-test split of the data and returns measures such as accuracy, sensitivity, specificity and AUC evaluated on the test set. We also plot the ROC curves using `plot_roc_curves_for_models`.\n",
        "\n",
        "We then define a function named `crossvalidation` which executes a cross validation procedure and prints the statistics of the results over the folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKuFUqwWW19K"
      },
      "source": [
        "#### Helper functions\n",
        "Helper function to plot customized ROC curves. Code inspired by [stackoverflow](https://stackoverflow.com/questions/42894871/how-to-plot-multiple-roc-curves-in-one-plot-with-legend-and-auc-scores-in-python)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWznfw3kW19K"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curves_for_models(models, test_x, test_y, save_png=False):\n",
        "    \"\"\"\n",
        "    Helper function to plot customized roc curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    models: dict\n",
        "        Dictionary of pretrained machine learning models.\n",
        "    test_x: list\n",
        "        Molecular fingerprints for test set.\n",
        "    test_y: list\n",
        "        Associated activity labels for test set.\n",
        "    save_png: bool\n",
        "        Save image to disk (default = False)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    fig:\n",
        "        Figure.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Below for loop iterates through your models list\n",
        "    for model in models:\n",
        "        # Select the model\n",
        "        ml_model = model[\"model\"]\n",
        "        # Prediction probability on test set\n",
        "        test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
        "        # Prediction class on test set\n",
        "        test_pred = ml_model.predict(test_x)\n",
        "        # Compute False postive rate and True positive rate\n",
        "        fpr, tpr, thresholds = metrics.roc_curve(test_y, test_prob)\n",
        "        # Calculate Area under the curve to display on the plot\n",
        "        auc = roc_auc_score(test_y, test_prob)\n",
        "        # Plot the computed values\n",
        "        ax.plot(fpr, tpr, label=(f\"{model['label']} AUC area = {auc:.2f}\"))\n",
        "\n",
        "    # Custom settings for the plot\n",
        "    ax.plot([0, 1], [0, 1], \"r--\")\n",
        "    ax.set_xlabel(\"False Positive Rate\")\n",
        "    ax.set_ylabel(\"True Positive Rate\")\n",
        "    ax.set_title(\"Receiver Operating Characteristic\")\n",
        "    ax.legend(loc=\"lower right\")\n",
        "    # Save plot\n",
        "    if save_png:\n",
        "        fig.savefig(f\"{DATA}/roc_auc\", dpi=300, bbox_inches=\"tight\", transparent=True)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIVbsWe1W19L"
      },
      "source": [
        "Helper function to calculate model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ga7evLL-W19L"
      },
      "outputs": [],
      "source": [
        "def model_performance(ml_model, test_x, test_y, verbose=True):\n",
        "    \"\"\"\n",
        "    Helper function to calculate model performance\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ml_model: sklearn model object\n",
        "        The machine learning model to train.\n",
        "    test_x: list\n",
        "        Molecular fingerprints for test set.\n",
        "    test_y: list\n",
        "        Associated activity labels for test set.\n",
        "    verbose: bool\n",
        "        Print performance measure (default = True)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple:\n",
        "        Accuracy, sensitivity, specificity, auc on test set.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prediction probability on test set\n",
        "    test_prob = ml_model.predict_proba(test_x)[:, 1]\n",
        "\n",
        "    # Prediction class on test set\n",
        "    test_pred = ml_model.predict(test_x)\n",
        "\n",
        "    # Performance of model on test set\n",
        "    accuracy = accuracy_score(test_y, test_pred)\n",
        "    sens = recall_score(test_y, test_pred)\n",
        "    spec = recall_score(test_y, test_pred, pos_label=0)\n",
        "    auc = roc_auc_score(test_y, test_prob)\n",
        "\n",
        "    if verbose:\n",
        "        # Print performance results\n",
        "        print(f\"Accuracy: {accuracy:.2}\")\n",
        "        print(f\"Sensitivity: {sens:.2f}\")\n",
        "        print(f\"Specificity: {spec:.2f}\")\n",
        "        print(f\"AUC: {auc:.2f}\")\n",
        "\n",
        "    return accuracy, sens, spec, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Z9TGyrMW19M"
      },
      "source": [
        " Helper function to fit a machine learning model on a random train-test split of the data and return the performance measures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFf5muROW19N"
      },
      "outputs": [],
      "source": [
        "def model_training_and_validation(ml_model, name, splits, verbose=True):\n",
        "    \"\"\"\n",
        "    Fit a machine learning model on a random train-test split of the data\n",
        "    and return the performance measures.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ml_model: sklearn model object\n",
        "        The machine learning model to train.\n",
        "    name: str\n",
        "        Name of machine learning algorithm: RF, SVM, ANN\n",
        "    splits: list\n",
        "        List of desciptor and label data: train_x, test_x, train_y, test_y.\n",
        "    verbose: bool\n",
        "        Print performance info (default = True)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple:\n",
        "        Accuracy, sensitivity, specificity, auc on test set.\n",
        "\n",
        "    \"\"\"\n",
        "    train_x, test_x, train_y, test_y = splits\n",
        "\n",
        "    # Fit the model\n",
        "    ml_model.fit(train_x, train_y)\n",
        "\n",
        "    # Calculate model performance results\n",
        "    accuracy, sens, spec, auc = model_performance(ml_model, test_x, test_y, verbose)\n",
        "\n",
        "    return accuracy, sens, spec, auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRtDgY_qW19N"
      },
      "source": [
        "**Preprocessing**: Split the data (will be reused for the other models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfiC0KdfW19O"
      },
      "outputs": [],
      "source": [
        "fingerprint_to_model = compound_df.fp.tolist()\n",
        "label_to_model = compound_df.active.tolist()\n",
        "\n",
        "# Split data randomly in train and test set\n",
        "# note that we use test/train_x for the respective fingerprint splits\n",
        "# and test/train_y for the respective label splits\n",
        "(\n",
        "    static_train_x,\n",
        "    static_test_x,\n",
        "    static_train_y,\n",
        "    static_test_y,\n",
        ") = train_test_split(fingerprint_to_model, label_to_model, test_size=0.2, random_state=SEED)\n",
        "splits = [static_train_x, static_test_x, static_train_y, static_test_y]\n",
        "print(\"Training data size:\", len(static_train_x))\n",
        "print(\"Test data size:\", len(static_test_x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqMXKhdXW19P"
      },
      "source": [
        "#### Random forest classifier\n",
        "\n",
        "We start with a random forest classifier, where we first set the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wguybg1VW19Q"
      },
      "source": [
        "We train the model on a random train-test split and plot the results."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Below we will create random forest, but we have to specify what number of trees to use (DECISION_TREES) - try different numbers and evaluate what works well for our current dataset.\n",
        "Note that the more trees, the longer this process will take (recommended max of 1000)"
      ],
      "metadata": {
        "id": "-4rB-EwQdP8F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgpSV6HbW19Q"
      },
      "outputs": [],
      "source": [
        "# Set model parameter for random forest\n",
        "param = {\n",
        "    \"n_estimators\": DECISION_TREES,  # number of trees to grows\n",
        "    \"criterion\": \"entropy\",  # cost function to be optimized for a split\n",
        "}\n",
        "model_RF = RandomForestClassifier(**param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i88r2CfiW19Q"
      },
      "outputs": [],
      "source": [
        "# Fit model on single split\n",
        "performance_measures = model_training_and_validation(model_RF, \"RF\", splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JgiF38CW19R"
      },
      "outputs": [],
      "source": [
        "# Initialize the list that stores all models. First one is RF.\n",
        "models = [{\"label\": \"Model_RF\", \"model\": model_RF}]\n",
        "# Plot roc curve\n",
        "plot_roc_curves_for_models(models, static_test_x, static_test_y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E2I_APOW19R"
      },
      "source": [
        "#### Support vector classifier\n",
        "Here we train a SVM with a radial-basis function kernel (also: squared-exponential kernel). \n",
        "For more information, see [sklearn RBF kernel](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.kernels.RBF.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "i4u90DvOW19R"
      },
      "outputs": [],
      "source": [
        "# Specify model\n",
        "model_SVM = svm.SVC(kernel=\"rbf\", C=1, gamma=0.1, probability=True)\n",
        "\n",
        "# Fit model on single split\n",
        "performance_measures = model_training_and_validation(model_SVM, \"SVM\", splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "nbsphinx-thumbnail"
        ],
        "id": "mCX1jKy6W19S"
      },
      "outputs": [],
      "source": [
        "# Append SVM model\n",
        "models.append({\"label\": \"Model_SVM\", \"model\": model_SVM})\n",
        "# Plot roc curve\n",
        "plot_roc_curves_for_models(models, static_test_x, static_test_y);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** In the function above, we set the function that SVM uses for the kernel \"trick\" to RBF. \n",
        "Evaluate the performance for, for example, a *linear* or a *poly* (polynomial) or *sigmoid* kernel."
      ],
      "metadata": {
        "id": "D8jp6F1VeRy7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4ji3wgmW19T"
      },
      "source": [
        "#### Neural network classifier\n",
        "The last approach we try here is a neural network model. We train an MLPClassifier (Multi-layer Perceptron classifier) with X layers, each with NUM_NEURONS neurons. As before, we do the crossvalidation procedure and plot the results. For more information on MLP, see [sklearn MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "fbgCckuMW19T"
      },
      "outputs": [],
      "source": [
        "# Specify model\n",
        "model_ANN = MLPClassifier(hidden_layer_sizes=(NUM_NEURONS_LAYER1, NUM_NEURONS_LAYER2), random_state=SEED)\n",
        "\n",
        "# Fit model on single split\n",
        "performance_measures = model_training_and_validation(model_ANN, \"ANN\", splits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUlGJNSWW19U"
      },
      "outputs": [],
      "source": [
        "# Append ANN model\n",
        "models.append({\"label\": \"Model_ANN\", \"model\": model_ANN})\n",
        "# Plot roc curve\n",
        "plot_roc_curves_for_models(models, static_test_x, static_test_y, True);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Evaluate a few different setups for number of layers and number of neurons. The more neurons and the more layers, the longer the training will take (use a max of 3 layers and a max of 10 neurons per layer).\n",
        "\n"
      ],
      "metadata": {
        "id": "bczEWMD4e5gW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kTDUdr6W19U"
      },
      "source": [
        "Your models should show good values for all measured values (see AUCs) and thus seem to be predictive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwHh5lXdW19U"
      },
      "source": [
        "#### Cross-validation\n",
        "\n",
        "Next, we will perform cross-validation experiments with the three different models.\n",
        "Therefore, we define a helper function for machine learning model training and validation in a cross-validation loop."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Why do we need cross-validation again? \n",
        "\n",
        "**Question**: What is the downside of using very little (e.g. 2) or a lot of folds (e.g. 100)?"
      ],
      "metadata": {
        "id": "OkrZwW6ZgTXd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9llfEBpW19U"
      },
      "outputs": [],
      "source": [
        "def crossvalidation(ml_model, df, n_folds=5, verbose=False):\n",
        "    \"\"\"\n",
        "    Machine learning model training and validation in a cross-validation loop.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    ml_model: sklearn model object\n",
        "        The machine learning model to train.\n",
        "    df: pd.DataFrame\n",
        "        Data set with SMILES and their associated activity labels.\n",
        "    n_folds: int, optional\n",
        "        Number of folds for cross-validation.\n",
        "    verbose: bool, optional\n",
        "        Performance measures are printed.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    None\n",
        "\n",
        "    \"\"\"\n",
        "    t0 = time.time()\n",
        "    # Shuffle the indices for the k-fold cross-validation\n",
        "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
        "\n",
        "    # Results for each of the cross-validation folds\n",
        "    acc_per_fold = []\n",
        "    sens_per_fold = []\n",
        "    spec_per_fold = []\n",
        "    auc_per_fold = []\n",
        "\n",
        "    # Loop over the folds\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        # clone model -- we want a fresh copy per fold!\n",
        "        fold_model = clone(ml_model)\n",
        "        # Training\n",
        "\n",
        "        # Convert the fingerprint and the label to a list\n",
        "        train_x = df.iloc[train_index].fp.tolist()\n",
        "        train_y = df.iloc[train_index].active.tolist()\n",
        "\n",
        "        # Fit the model\n",
        "        fold_model.fit(train_x, train_y)\n",
        "\n",
        "        # Testing\n",
        "\n",
        "        # Convert the fingerprint and the label to a list\n",
        "        test_x = df.iloc[test_index].fp.tolist()\n",
        "        test_y = df.iloc[test_index].active.tolist()\n",
        "\n",
        "        # Performance for each fold\n",
        "        accuracy, sens, spec, auc = model_performance(fold_model, test_x, test_y, verbose)\n",
        "\n",
        "        # Save results\n",
        "        acc_per_fold.append(accuracy)\n",
        "        sens_per_fold.append(sens)\n",
        "        spec_per_fold.append(spec)\n",
        "        auc_per_fold.append(auc)\n",
        "\n",
        "    # Print statistics of results\n",
        "    print(\n",
        "        f\"Mean accuracy: {np.mean(acc_per_fold):.2f} \\t\"\n",
        "        f\"and std : {np.std(acc_per_fold):.2f} \\n\"\n",
        "        f\"Mean sensitivity: {np.mean(sens_per_fold):.2f} \\t\"\n",
        "        f\"and std : {np.std(sens_per_fold):.2f} \\n\"\n",
        "        f\"Mean specificity: {np.mean(spec_per_fold):.2f} \\t\"\n",
        "        f\"and std : {np.std(spec_per_fold):.2f} \\n\"\n",
        "        f\"Mean AUC: {np.mean(auc_per_fold):.2f} \\t\"\n",
        "        f\"and std : {np.std(auc_per_fold):.2f} \\n\"\n",
        "        f\"Time taken : {time.time() - t0:.2f}s\\n\"\n",
        "    )\n",
        "\n",
        "    return acc_per_fold, sens_per_fold, spec_per_fold, auc_per_fold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXr--WJBW19V"
      },
      "source": [
        "**Cross-validation**\n",
        "\n",
        "We now apply cross-validation and show the statistics for all three ML models. In real world conditions, cross-validation usually applies 5 or more folds, but for the sake of performance we will reduce it to 3. You can change the value of `N_FOLDS` in this cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKOimaIsW19W"
      },
      "outputs": [],
      "source": [
        "N_FOLDS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tta00I9AW19W"
      },
      "source": [
        "_Note_: Next cell takes long to execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVWx6rhKW19W"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    print(\"\\n======= \")\n",
        "    print(f\"{model['label']}\")\n",
        "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_xeAWIKW19W"
      },
      "source": [
        "We look at the cross-validation performance for molecules encoded using Morgan fingerprint and not MACCS keys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCmqogTZW19X"
      },
      "outputs": [],
      "source": [
        "# Reset data frame\n",
        "compound_df = chembl_df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0l9_TOdSW19X"
      },
      "outputs": [],
      "source": [
        "# Adjust FINGERPRINT to use morgan with a radius of 3 instead of the current fingerprint.\n",
        "compound_df[\"fp\"] = compound_df[\"smiles\"].apply(smiles_to_fp, args=(FINGERPRINT,))\n",
        "compound_df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERYMjxbfW19X"
      },
      "source": [
        "_Note_: Next cell takes long to execute\n",
        "\n",
        "**Question**: Why?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQzX9ghlW19X"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    print(\"\\n=======\")\n",
        "    print(model[\"label\"])\n",
        "    crossvalidation(model[\"model\"], compound_df, n_folds=N_FOLDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Now look back and summarize the steps you've taken in creating and evaluating your models, starting at the input collection. Are you happy with the results? Are there elements that are missing?  \n"
      ],
      "metadata": {
        "id": "bdnUliCaifNZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiB5iL8yW19Y"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "* Which model performed best on our data set and why?\n",
        "    * All three models perform (very) well on our dataset. The best models are the random forest and support vector machine models which showed a mean AUC of about 90%. Our neural network showed slightly lower results. \n",
        "    * There might be several reasons that random forest and support vector machine models performed best. Our dataset might be easily separable in active/inactive with some simple tree-like decisions or with the radial basis function, respectively. Thus, there is not such a complex pattern in the fingerprints to do this classification.\n",
        "    * A cause for the slightly poorer performance of the ANN could be that there was simply too few data to train the model on.\n",
        "    * Additionally, it is always advisable to have another external validation set for model evaluation.  \n",
        "* Was MACCS the right choice?\n",
        "    * Obviously, MACCS was good to start training and validating models to see if a classification is possible. \n",
        "    * However, MACCS keys are rather short (166 bit) compared to others (2048 bit), as for example Morgan fingerprint. As shown in the last simulation, having longer fingerprint helps the learning process. All tested models performed slightly better using Morgan fingerprints (see mean AUC increase).\n",
        "\n",
        "\n",
        "### Where can we go from here?\n",
        "\n",
        "* We successfully trained several models. \n",
        "* The next step could be to use these models to do a classification with an unknown screening dataset to predict novel potential EGFR inhibitors.\n",
        "* An example for a large screening data set is e.g. [MolPort](https://www.molport.com/shop/database-download) with over 7 million compounds.\n",
        "* Our models could be used to rank the MolPort compounds and then further study those with the highest predicted probability of being active.\n",
        "* For such an application, see also the [TDT Tutorial](https://github.com/sriniker/TDT-tutorial-2014) developed by S. Riniker and G. Landrum, where they trained a fusion model to screen [eMolecules](https://www.emolecules.com/) for new anti-malaria drugs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJliCe3UW19Y"
      },
      "source": [
        "### References\n",
        "\n",
        "* \"Fingerprints in the RDKit\" [slides](https://www.rdkit.org/UGM/2012/Landrum_RDKit_UGM.Fingerprints.Final.pptx.pdf), G. Landrum, RDKit UGM 2012\n",
        "* Extended-connectivity fingerprints (ECFPs): Rogers, David, and Mathew Hahn. \"Extended-connectivity fingerprints.\" [_Journal of chemical information and modeling_ 50.5 (2010): 742-754.](https://doi.org/10.1021/ci100050t)\n",
        "* Machine learning (ML):\n",
        "  * Random forest (RF): Breiman, L. \"Random Forests\". [_Machine Learning_ **45**, 5–32 (2001).](https://link.springer.com/article/10.1023%2FA%3A1010933404324)\n",
        "  * Support vector machines (SVM): Cortes, C., Vapnik, V. \"Support-vector networks\". [_Machine Learning_ **20**, 273–297 (1995).](https://link.springer.com/article/10.1007%2FBF00994018)\n",
        "  * Artificial neural networks (ANN): Van Gerven, Marcel, and Sander Bohte. \"Artificial neural networks as models of neural information processing.\" [_Frontiers in Computational Neuroscience_ 11 (2017): 114.](https://doi.org/10.3389/fncom.2017.00114)\n",
        "* Performance: \n",
        "  * Sensitivity and specificity ([Wikipedia](https://en.wikipedia.org/wiki/Sensitivity_and_specificity))\n",
        "  * ROC curve and AUC ([Wikipedia](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve))\n",
        "* See also [github notebook by B. Merget](https://github.com/Team-SKI/Publications/tree/master/Profiling_prediction_of_kinase_inhibitors) from [*J. Med. Chem.*, 2017, 60, 474−485](https://pubs.acs.org/doi/10.1021/acs.jmedchem.6b01611) \n",
        "* Activity cutoff $pIC_{50} = 6.3$ (i.e. 500 nM) is used in this practical\n",
        "  * Profiling Prediction of Kinase Inhibitors: Toward the Virtual Assay [<i>J. Med. Chem.</i> (2017), <b>60</b>, 474-485](https://doi.org/10.1021/acs.jmedchem.6b01611)\n",
        "  * Notebook accompanying the publication mentioned before: [Notebook](https://github.com/Team-SKI/Publications/blob/master/Profiling_prediction_of_kinase_inhibitors/Build_ABL1_model.ipynb)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "toc-autonumbering": true,
    "colab": {
      "name": "Ligand_based_machine_learning.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
